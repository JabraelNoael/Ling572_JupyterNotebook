# -*- coding: utf-8 -*-
"""Copy of text_classification_and_regression_assignment_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dsu5Ya6-6ydubIOZa7ufcA8exaLLzf_O

## Millikan electron experiment (Regression, Newman Computational Physics, Exc 3.8  p.123 )

This is worth 3 points.

The whole source of excitement about the photolectric effect was that it is governed by a very simple equation, whose verification turned out to be very strong evidence that light consists of particles (let's call them photons).


$$
\text{V} = \frac{h}{e} \nu - \Phi
$$

Read this as V (Voltage) = $\frac{h}{e}$ (Planck's Constant divided by the charge of an electron) times
$\nu$ (the frequency of the light) minus $\Phi$ (known as the work function of the surface).
So an electron gets kicked up from the surface by a photon, loses a little energy ($\Phi$)
because it takes some work to pry it loose, and then has energy (V) proportional to the
light's wavelength.

The experimental setup was such that the work function of the surface was constant throughout  the experiment.

Here is Millikan's data, measuring the voltage produced by single photons of light
at various frequencies. He won the Nobel Prize for these results, which confirmed
predictions made in Einstein's 1905 work on the photoelectric effect, for which Einstein also
won the Nobel Prize.
"""

import pandas as pd

github_url ='https://raw.githubusercontent.com/gawron/python-for-social-science/master/'
data_path = 'text_classification/data/'
url_dir = github_url + data_path
data_file = "millikan.txt"

###############################################################################
#
#                    D a t a
#
###############################################################################
e_charge = 1.602e-19
df = pd.read_csv(url_dir + data_file,header=None,delimiter= " ",names=("Frequency","Voltage"))

df

"""
1.  The equation says the Frequency and the voltage should be related by a linear relation. Assuming the data do obey this linear relation, use Scikit learn's linear regression implementation to find $\frac{h}{e}$ and $\Phi$ from these data points.  

2.  Then given that the charge of an electron  is $1.602e^{-19}$, estimate Planck's constant. This is one of the most important constants in Physics.  Look it up to see how good your estimate is.  Compute your percentage of error.

3.  Draw a scatter plot of the 6 data points.  Use the results of your linear regression to plot the  line that is the best fit to these points in the same plot.  You will need to make a fairly arbitrary decision about the aspect of the figure (ratio of height to width), since the x and y axis represent different units with very different orders of magnitude.

4.  What is the value of $\Phi$?  Hint:  It's work.  It shouldn't be negative."""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
#1
voltages = df["Voltage"].values
frequencies = df["Frequency"].values.reshape(-1,1)
model = LinearRegression()
model.fit(frequencies, voltages)
slope = model.coef_[0]
intercept = model.intercept_
print(f"1. Regression implementation\n{voltages} = voltages\n{frequencies} = frequencies\n{slope} = slope\n{intercept} = intercept") #Why can't I transpose 'frequencies' ?
#2
h = 6.62607015*(10**(-34))
hEstimate = slope * e_charge
percentError = abs(hEstimate - h) / h * 100
print(f"\n2. Planck's Constant estimation and Percent Error\n{h} = Planck's Constant\n{hEstimate} = Planck's Constant Estimate from Slope & Electron Charge\n{percentError} = Percent error in my estimation from Planck's Constant")
#3
print("\n3. Scatter Plot")
plt.figure(figsize=(10, 6))
plt.scatter(frequencies, voltages, color="blue", label="Data")
plt.plot(frequencies, model.predict(frequencies), color="red",
         label=f"Fit: $V = {slope:.2e}ν - {abs(intercept):.2f}$")
plt.xlabel("Frequency (Hz)")
plt.ylabel("Voltage (V)")
plt.title("Millikan Electron Experiment")
plt.legend()
plt.grid()
plt.show()
#4
workFN = abs(intercept)
print(f"\n4. Work function (Φ): {workFN:.2f} eV")

"""## 1985 Auto Imports Database  Exercise  (Regression)

This problem is worth 7 points.

UCI Data sets

Schlimmer, J. (1985). Automobile Dataset. UCI Machine Learning Repository. https://doi.org/10.24432/C5B01C.

-- Creator/Donor: Jeffrey C. Schlimmer (Jeffrey.Schlimmer@a.gp.cs.cmu.edu)
-- Date: 19 May 1987
-- Sources:

     1) 1985 Model Import Car and Truck Specifications, 1985 Ward's
        Automotive Yearbook.
     2) Personal Auto Manuals, Insurance Services Office, 160 Water
        Street, New York, NY 10038
     3) Insurance Collision Report, Insurance Institute for Highway
        Safety, Watergate 600, Washington, DC 20037

This exercise involves a technique called **cross-validation**: k-fold cross-validation means
that you do k different train test splits, each time using a different portion of the data for the
test set.  The way this typically works is that by the end of the k splits, each item in the dataset
has had a chance to be in the test set (that's the way the scikit_learn `KFold` function works).
Intuitively this means that if there are items in the data that are particularly hard, the learner will
get a crack at predicting those items without having seen them during training.

Here is how cross validation on a small dataset of size 7 works if we require there to
be 7 splits:
"""

import numpy as np
from sklearn.model_selection import KFold
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6],[7,5]])
y = np.array([1, 2, 1, 2, 1, 2, 2])
rs = KFold(n_splits=7)

for i, (train_index, test_index) in enumerate(rs.split(X)):
    print(f"Fold {i}:")
    print(f"  Train: indices = {train_index}")
    print(f"  Test:  indices = {test_index}")

"""**General idea of the assignment: Reproduce the regression result from Kibler et al.**

Kibler, D., Aha, D. W., & Albert, M. (1989).  Instance-based prediction of real-valued attributes.  *Computational Intelligence 5* pp.  51--57.

**Predicted price of car using all Numeric and Boolean attributes.**

Their Method: an instance-based learning (IBL) algorithm derived from a localized k-nearest neighbor algorithm. You won't build the IBL system.  You will just implement the system they  compared it with: a standard  linear regression model. Note that they discarded all instances with missing attribute values.  You should do the same.  This resulted in a training set of 159 instances.

The Idea: Train 159 models. In each case, train with 158 instances.  Predict the price of  the  159th instance and save that in an array called `predictions`.  Each time you train,. a different row of the data should be held out. to be used as test data; scikit learn's KFold should help.


Results:

Percent Average Deviation Error, also known as MAPE ("mean absolute percentage error")  of Prediction from Actual

1. 11.84% for the IBL algorithm
2. 14.12% for the resulting linear regression equation

#### Attribute information

1. symboling:                -3, -2, -1, 0, 1, 2, 3.
2. normalized-losses:        continuous from 65 to 256.
3. make:                     alfa-romero, audi, bmw, chevrolet, dodge, honda,
                           isuzu, jaguar, mazda, mercedes-benz, mercury,
                           mitsubishi, nissan, peugot, plymouth, porsche,
                           renault, saab, subaru, toyota, volkswagen, volvo
4. fuel-type:                diesel, gas.
5. aspiration:               std, turbo.
6. num-of-doors:             four, two.
7. body-style:               hardtop, wagon, sedan, hatchback, convertible.
8. drive-wheels:             4wd, fwd, rwd.
9. engine-location:          front, rear.
10. wheel-base:               continuous from 86.6 120.9.
11. length:                   continuous from 141.1 to 208.1.
12. width:                    continuous from 60.3 to 72.3.
13. height:                   continuous from 47.8 to 59.8.
14. curb-weight:              continuous from 1488 to 4066.
15. engine-type:              dohc, dohcv, l, ohc, ohcf, ohcv, rotor.
16. num-of-cylinders:         eight, five, four, six, three, twelve, two.
17. engine-size:              continuous from 61 to 326.
18. fuel-system:              1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.
19. bore:                     continuous from 2.54 to 3.94.
20. stroke:                   continuous from 2.07 to 4.17.
21. compression-ratio:        continuous from 7 to 23.
22. horsepower:               continuous from 48 to 288.
23. peak-rpm:                 continuous from 4150 to 6600.
24. city-mpg:                 continuous from 13 to 49.
25. highway-mpg:              continuous from 16 to 54.
26. price:                    continuous from 5118 to 45400.

#### Missing values info

Missing values are denoted by '?'.

#### Loading the data
"""

import pandas as pd


#import os
#print(os.getcwd())


cols = "symboling normalized-losses make fuel-type aspiration num-of-doors body-style" +\
       " drive-wheels engine-location wheel-base length width height curb-weight" +\
       " engine-type num-of-cylinders engine-size fuel-system bore stroke" +\
       " compression-ratio horsepower peak-rpm city-mpg highway-mpg price"
cols = cols.split()

data_file = "imports-85.data"
df = pd.read_csv(url_dir + data_file,sep=",",header=None,names=cols)

"""#### Instructions

Build a regression model predict car price from other numeric and Boolean variables.  Here are some specific things to do:

1.  Select the Numeric and Boolean attributes you will use.  Note that the data types  you get when you read in the data are inferred by pandas and they may not be correct. Think about whether the data types pandas uses for each column are appropriate for the information being represented in that column.  Also note: There are obviously no Boolean data types directly represented in the data, but you can create some Boolean type columns using `.str.get_dummies()`. For example, try executing `df["body-style"].str.get_dummies()`.  This returns a Dataframe with the Boolean columns derived from the categorial `"body_style"` column. You use `pd.merge` to merge these new columns into `df`.
2.  Do any data type conversions you need to do to use the attributes sensibly.  Note:
    One of the best ways to do your type conversions is to do them while
    reading in the data with `read_csv` (called above).  It is recommended that you
    read the docs for function by googling "pandas read_csv".
3.  Discard instances with missing values. Note that missing values are denoted by
    '?', not a convention known to pandas.   Again "read_csv" can help.
4.  Build a linear regression model to predict price. Since you are trying to reproduce
    Kibler at al. you must do the training/test splits the way they do: Use all but one
    row of data for training on each train-test split, with the held out row
    used as test data. Do N train-test splits where N is the number of rows. A different
    row should be held out each time (so you can't choose the held out row by random
    selection).
5.  Evaluation.  Compute root mean squared error.  Also compute Average
    Deviation Error of Prediction from Actual (for comparison with Kibler et al.)  You can
    use scikit_learn's `mean_absolute_percentage_error` for this.  Please pay attention to
    the right argument order.  Call the 1D array of the predictions you made during your
    159 train-test splits `predictions`. Conceptually, `predictions` and `df["price"]` are
    the arguments of `mean_absolute_percentage_error` (MAPE) and
    `root_mean_squared_error` (RSME).  Note You should be able to build a linear regression
    model that beats the 14.12% MAPE score for a regression model reported by
    Kibler et al.
6.  Try different feature sets. Is it sometimes a good idea to leave a feature out?
    Yes! Report the results for your best model.  Note: you don't have to and should not
    try all possible subsets of the columns of `df`.  Just try a model with all possible
    numerical and Boolean columns as features and then look at ways to improve it by
    pruning some columns/features.  You don't have to show all your experiments.
    Just your best model. The code that produced it.  And the MAPE and RSME
    scores for 159 predictions you made with that model (one model, two numbers).
7.  For your best model report your most important positive feature (most likely to inflate price) and your
    most important negative feature (most likely to lower price). Find the 5 features that matter the least
    in determining price.
8.  To help you interpret your results, implement and evaluate a baseline model
    for comparison.  The baseline should always predict
    the mean price of the entire data set.  Find
    the MAPE and RSME for the baseline model.  How does the RSME of this baseline model
    compare with the STD of the price column?
9.  Optionally, try out sklearn's `linear_model.LogisticRegression` on the problem.  
    To do this you   will need to scale the data using sklearn's
    `preprocessing.StandardScaler`. Read [the
    docs](https://scikit-learn.org/1.5/modules/preprocessing.html)
    because this is implemented as an sklearn Transformer.  What effect does using
    Logistic Regression have?

Data types as read in by `read_csv`.  Recall that columns containing strings
will by default be given the data type `object`.
"""

df.info()

"""A look at the regression variable:"""

df["price"]

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
#1
data_file = "imports-85.data"
df = pd.read_csv(url_dir+data_file, header=None, names=cols, na_values="?")
categorical_cols = ['fuel-type', 'aspiration', 'body-style', 'drive-wheels']
dummy_dfs = [pd.get_dummies(df[col], prefix=col) for col in categorical_cols]
df_processed = pd.concat([df, *dummy_dfs], axis=1).drop(columns=categorical_cols)
#2
numeric_cols = ['normalized-losses', 'bore', 'stroke', 'horsepower', 'peak-rpm', 'price']
for col in numeric_cols:
    df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
#3
df_processed = df_processed.dropna(subset=['price'] + numeric_cols)
#4
X = df_processed.select_dtypes(include=['number']).drop(columns=['price'])
y = df_processed['price']

kf = KFold(n_splits=len(df_processed), shuffle=False)  # No random selection
predictions = []
actuals = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    model = LinearRegression().fit(X_train, y_train)
    predictions.append(model.predict(X_test)[0])
    actuals.append(y_test.iloc[0])
#5
mape = mean_absolute_percentage_error(actuals, predictions) * 100
rmse = np.sqrt(mean_squared_error(actuals, predictions))
print(f"MAPE: {mape:.2f}% (Target < 14.12%)")
print(f"RMSE: {rmse:.2f}")
#6
final_model = LinearRegression().fit(X, y)
coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': final_model.coef_})

print("Most positive feature:", coefficients.loc[coefficients['Coefficient'].idxmax()])
print("Most negative feature:", coefficients.loc[coefficients['Coefficient'].idxmin()])

print("5 Least important features:")
print(coefficients.reindex(coefficients['Coefficient'].abs().sort_values().index).head(5))
#7
baseline_pred = np.full_like(actuals, y.mean())
baseline_mape = mean_absolute_percentage_error(actuals, baseline_pred) * 100
baseline_rmse = np.sqrt(mean_squared_error(actuals, baseline_pred))

print(f"Baseline MAPE: {baseline_mape:.2f}%")
print(f"Baseline RMSE: {baseline_rmse:.2f} (Price STD: {y.std():.2f})")
#8
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

y_binary = (y > y.median()).astype(int)
log_reg = LogisticRegression(max_iter=1000).fit(X_scaled, y_binary)
print("Logistic Regression Accuracy:", log_reg.score(X_scaled, y_binary))
#9 (Attempt if spare time after "Text Classification")

"""## Text Classification

### General idea

Work through the Insults Detection notebook about text classification and
insult detection. Focus on the use of `scikit_learn`, especially the
`TfidfVectorizer`. For this assignment you will be turning in the Python notebook (extension `.ipynb`, **not** a `.py` file).  Turn in this notebook with all the code needed to run your classifier.  If it doesn't run, your score will suffer.

Try two different classifiers on the movie review data, the one used in the textbook, an SVM called `LinearSVC`, and  the Bernoulli Naive Bayes model used above.

### Instructions and point values

1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.  2 points
2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_vectorize_and_fit` defined above useful, but you will need to modify it.  2 points.
3.  Discuss which of the two classifiers does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting). 3 points.
4. Do a new training/test split on the data and train and test an SVM model.  Choose one false positive and one false negative from the test set.  Call these documents $j$ and $k$ and call their functional margins $c_j$ and $c_k$ (see the SVM notebook).  Find

$$
\frac{c_{j}}{c_{max}-c_{min}}
$$

and

$$
\frac{c_{k}}{c_{max}-c_{min}},
$$

where $c_{max}$ and $c_{min}$ are the maximum and minimum functional margins for the training set.  Are documents $j$ and $k$ misclassified with high confidence?  Of course getting credit for this part means submitting the code you used to compute these quantities.  For the computation of functional margins, it will be convenient to relabel positive and negative classes 1 and -1 respectively. 5 points

#### Help with getting the movie reviews data.

Execute the next two cells to get the movie review data.
"""

import nltk
nltk.download('movie_reviews')

from nltk.corpus import movie_reviews as mr

def get_file_strings (corpus, file_ids):
    return [corpus.raw(file_id) for file_id in file_ids]

data = dict(pos = mr.fileids('pos'),
            neg = mr.fileids('neg'))

pos_file_ids = data['pos']
neg_file_ids = data['neg']

# Get all the positive and negative reviews.
pos_file_reviews = get_file_strings (mr, pos_file_ids)
neg_file_reviews = get_file_strings (mr, neg_file_ids)

"""Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.
You could code that up.

What you'd really like to do is use `split_vectorize_and_fit`, defined above, which does a lot of the work for you.

But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have
one sequence of positive documents  and another sequence of negative documents.  

So you will need to turn those two sequences into a sequence of documents and a sequence of labels
because that's what `split_vectorize_and_fit` wants.  You also want the doc sequence
to contain a random mixture of positive and negative documents, because some machine
learning algorithms are sensitive to the order in which training data is presented to
them.

The next cell does **not** do that for you.  But it illustrates an approach using
two sets of English letters in place of two sets of English documents.
"""

# Lets work on letters instead of documents
# There are 2 classes, letters from the first half of the
# alphabet ('f') and letters frmm the last half ('l')

from random import shuffle
from string import ascii_lowercase

#Class 1 of the letters: the f_lets
f_lets = ascii_lowercase[:13]
print(f_lets)
#Class2 of the letters: the l_lets
l_lets = ascii_lowercase[13:]
print(l_lets)

# Now get pairs of letters and labels
f_pairs = [(let,'f') for let in f_lets]
l_pairs = [(let,'l') for let in l_lets]

###########  Shuffling  ###########################
# Way too orderly, the classes arent mixed yet.
data = f_pairs + l_pairs
shuffle(data)
###################  Now they're shuffled! ###############

# Separate the letters from their labels
lets, lbls = zip(*data)
print(lets)
print(lbls)

from sklearn.model_selection import train_test_split

#1
documents = pos_file_reviews + neg_file_reviews
labels = [1] * len(pos_file_reviews) + [0] * len(neg_file_reviews)

shuffled_idx = np.random.permutation(len(documents))
documents = [documents[i] for i in shuffled_idx]
labels = [labels[i] for i in shuffled_idx]
#2
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, precision_score, recall_score

vectorizer = TfidfVectorizer(max_features=5000)
svm = LinearSVC()
nb = BernoulliNB()

svm_metrics = {"accuracy": [], "precision": [], "recall": []}
nb_metrics = {"accuracy": [], "precision": [], "recall": []}

for _ in range(10):
    X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2)

    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    svm.fit(X_train_vec, y_train)
    svm_pred = svm.predict(X_test_vec)
    svm_metrics["accuracy"].append(accuracy_score(y_test, svm_pred))
    svm_metrics["precision"].append(precision_score(y_test, svm_pred))
    svm_metrics["recall"].append(recall_score(y_test, svm_pred))

    nb.fit(X_train_vec, y_train)
    nb_pred = nb.predict(X_test_vec)
    nb_metrics["accuracy"].append(accuracy_score(y_test, nb_pred))
    nb_metrics["precision"].append(precision_score(y_test, nb_pred))
    nb_metrics["recall"].append(recall_score(y_test, nb_pred))

print("SVM Metrics (Avg):")
print(f"Accuracy: {np.mean(svm_metrics['accuracy']):.3f}")
print(f"Precision: {np.mean(svm_metrics['precision']):.3f}")
print(f"Recall: {np.mean(svm_metrics['recall']):.3f}\n")

print("Naive Bayes Metrics (Avg):")
print(f"Accuracy: {np.mean(nb_metrics['accuracy']):.3f}")
print(f"Precision: {np.mean(nb_metrics['precision']):.3f}")
print(f"Recall: {np.mean(nb_metrics['recall']):.3f}")
#3
y_train_svm = np.array(y_train) * 2 - 1
y_test_svm = np.array(y_test) * 2 - 1

svm.fit(X_train_vec, y_train_svm)
decisions = svm.decision_function(X_test_vec)

misclassified = np.where(svm.predict(X_test_vec) != y_test_svm)[0]
fp_idx = [i for i in misclassified if y_test_svm[i] == -1]
fn_idx = [i for i in misclassified if y_test_svm[i] == 1]

c_max, c_min = np.max(decisions), np.min(decisions)
fp_margin = decisions[fp_idx[0]] / (c_max - c_min) if fp_idx else None
fn_margin = decisions[fn_idx[0]] / (c_max - c_min) if fn_idx else None

print(f"False Positive Margin: {fp_margin:.3f}" if fp_idx else "No FP found")
print(f"False Negative Margin: {fn_margin:.3f}" if fn_idx else "No FN found")
#4
documents = pos_file_reviews + neg_file_reviews
labels = [1] * len(pos_file_reviews) + [0] * len(neg_file_reviews)

combined = list(zip(documents, labels))
shuffle(combined)
documents, labels = zip(*combined)

X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

svm_model = LinearSVC()
svm_model.fit(X_train_vec, np.array(y_train) * 2 - 1)
decisions = svm_model.decision_function(X_test_vec)

misclassified_indices = np.where(svm_model.predict(X_test_vec) != np.array(y_test) * 2 - 1)[0]
fp_idx = [i for i in misclassified_indices if (np.array(y_test) * 2 - 1)[i] == -1]
fn_idx = [i for i in misclassified_indices if (np.array(y_test) * 2 - 1)[i] == 1]

c_max = np.max(svm_model.decision_function(X_train_vec))
c_min = np.min(svm_model.decision_function(X_train_vec))


if fp_idx and fn_idx :
    j_idx = fp_idx[0]
    k_idx = fn_idx[0]
    c_j = decisions[j_idx]
    c_k = decisions[k_idx]
    print("c_j / (c_max - c_min):", c_j / (c_max - c_min))
    print("c_k / (c_max - c_min):", c_k / (c_max - c_min))
else:
    print("Not enough false positive or false negative instances found.")